{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10cb6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertModel, BertConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b17a61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b0bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_tag(idx, entities, type_labels):\n",
    "    \n",
    "    for entity in entities:\n",
    "        if idx in entity['span']:\n",
    "            return type_labels.index(entity['type'])\n",
    "    \n",
    "    if idx == None:\n",
    "        return -100\n",
    "        \n",
    "    if set_none:\n",
    "        return type_labels.index('None')\n",
    "    else:\n",
    "        return -100\n",
    "\n",
    "def entity_to_tag(encoding, entities, type_labels):\n",
    "    \n",
    "    y = encoding.word_ids[1:]\n",
    "    y = [idx_to_tag(index, entities, type_labels) for index in y]\n",
    "            \n",
    "    return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b861417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "class MyData(Dataset):\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.df = None\n",
    "        self.type_labels = None\n",
    "        self.intent_labels = None\n",
    "\n",
    "        with open(filename) as file:\n",
    "            data = json.load(file)\n",
    "            self.df = pd.DataFrame(data)\n",
    "            types = pd.json_normalize(data, record_path='entities')\n",
    "            _, type_labels = pd.factorize(types['type'])\n",
    "            self.type_labels = type_labels.to_numpy().tolist()\n",
    "\n",
    "        self.df['intent'], intent_labels = pd.factorize(self.df['intent'])\n",
    "        self.intent_labels = intent_labels.to_numpy().tolist()\n",
    "        if set_none:\n",
    "            self.type_labels.append('None')\n",
    "        \n",
    "        \n",
    "        self.y_label = self.df['intent']\n",
    "        self.x = self.df['sentence']\n",
    "        self.y = self.df['entities']\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx], (self.y_label[idx], self.y[idx])\n",
    "    \n",
    "\n",
    "class MyCollate(object):\n",
    "    \n",
    "    def __init__(self, type_labels, tokenizer):\n",
    "        self.type_labels = type_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        x = [entry[0] for entry in batch]\n",
    "        y = [entry[1][1] for entry in batch]\n",
    "        y_label = [entry[1][0] for entry in batch]\n",
    "        \n",
    "        out_y = []\n",
    "        \n",
    "        out_x = self.tokenizer(x, padding=True, return_tensors='pt')\n",
    "        for index in range(len(x)):\n",
    "            out_y.append(entity_to_tag(out_x[index], y[index], self.type_labels))\n",
    "        \n",
    "        y_label = torch.tensor(y_label)\n",
    "        out_y = torch.tensor(out_y)\n",
    "        return out_x, (y_label, out_y)    \n",
    "\n",
    "myData = MyData('./archive/slurp/train.json')\n",
    "myCollate = MyCollate(myData.type_labels, myData.tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(myData, batch_size=64, shuffle=True, collate_fn=myCollate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f466a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name, num_intent_classes, num_tag_classes):\n",
    "        super(Model, self).__init__()\n",
    "              \n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.config = BertConfig.from_pretrained(model_name)       \n",
    "        self.intent_classifier = nn.Linear(in_features=self.config.hidden_size, out_features=num_intent_classes)   \n",
    "        \n",
    "        torch.nn.init.xavier_normal_(self.intent_classifier.weight.data)\n",
    "        torch.nn.init.uniform_(self.intent_classifier.bias.data)\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        output = self.bert(**inputs)\n",
    "        intent = self.intent_classifier(output.last_hidden_state[:,0])\n",
    "        intent = F.sigmoid(intent)\n",
    "\n",
    "        return intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc06194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_y_label = [0] * len(myData.intent_labels)\n",
    "count_y_tag = [0] * len(myData.type_labels)\n",
    "\n",
    "for train_x, (train_y_label, train_y_tag) in train_dataloader:\n",
    "    for idx in range(len(train_y_label)):\n",
    "        count_y_label[train_y_label[idx]] += 1 \n",
    "        for k in range(len(train_y_tag[idx])):\n",
    "            if train_y_tag[idx][k] == -100:\n",
    "                continue\n",
    "            count_y_tag[train_y_tag[idx][k]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6ff18937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "num_epoches = 5\n",
    "epoch = 0\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "myModel = Model(\"bert-base-cased\", len(myData.intent_labels), len(myData.type_labels))\n",
    "\n",
    "intent_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=myModel.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "intent_error = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f51abbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n",
      "intent error: 4.476026\n",
      "Epoch : 2\n",
      "intent error: 4.291128\n",
      "Epoch : 3\n",
      "intent error: 4.182344\n",
      "Epoch : 4\n",
      "intent error: 4.107389\n",
      "Epoch : 5\n",
      "intent error: 4.047712\n"
     ]
    }
   ],
   "source": [
    "myModel.train()\n",
    "myModel.to(device)\n",
    "\n",
    "while epoch < num_epoches:\n",
    "     \n",
    "    print('Epoch : %d' % (epoch+1))\n",
    "    intent_error.append(0)\n",
    "    num_iter = 0\n",
    "    \n",
    "    for train_x, (train_y_label, train_y_tag) in train_dataloader:\n",
    "        \n",
    "        train_x = train_x.to(device)\n",
    "        train_y_label = train_y_label.to(device)\n",
    "        \n",
    "        \n",
    "        intent = myModel(train_x)       \n",
    "        \n",
    "        intent_loss = intent_criterion(intent, train_y_label)       \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        intent_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        num_iter+=1\n",
    "        \n",
    "        intent_error[-1] += intent_loss.item()        \n",
    "    \n",
    "    intent_error[-1] /= num_iter\n",
    "    print('intent error: %f' % (intent_error[-1]))\n",
    "          \n",
    "    epoch+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8919899d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime_query calendar_query\n"
     ]
    }
   ],
   "source": [
    "myModel.eval()\n",
    "\n",
    "test_input = \"what time is it?\"\n",
    "test_input_2 = \"who are you\"\n",
    "test_input = myData.tokenizer(test_input, padding=True, return_tensors='pt').to(device)\n",
    "test_input_2 = myData.tokenizer(test_input_2, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "intent = myModel(test_input)\n",
    "intent2 = myModel(test_input_2)\n",
    "print(myData.intent_labels[intent.argmax()], myData.intent_labels[intent2.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2462759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_slot(tokenizer, token_tag, tokens):  \n",
    "    start_idx = None\n",
    "    prev_tag = 'None'\n",
    "    ret = []  \n",
    "    \n",
    "    for idx in range(len(token_tag)):\n",
    "        \n",
    "        if token_tag[idx] == prev_tag:\n",
    "            continue\n",
    "        \n",
    "        elif prev_tag == 'None':\n",
    "            start_idx = idx\n",
    "        \n",
    "        elif prev_tag != 'None':\n",
    "            \n",
    "            if token_tag[idx] == 'None':\n",
    "                ret.append({prev_tag : tokenizer.convert_tokens_to_string(tokens[start_idx:idx])})\n",
    "            \n",
    "            else:\n",
    "                ret.append({prev_tag : tokenizer.convert_tokens_to_string(tokens[start_idx:idx])})\n",
    "                start_idx = idx\n",
    "        \n",
    "        prev_tag = token_tag[idx]\n",
    "        \n",
    "        \n",
    "    return ret   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63106469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6555e+00,  1.1889e-03, -6.2424e-01,  1.3413e+00, -1.8278e-01,\n",
      "         -2.9840e-01,  3.1567e-01,  1.1413e+00,  3.0547e-01, -1.1709e-01,\n",
      "          2.3932e-01,  1.6241e+00,  5.2523e-01,  7.2117e-01, -6.0465e-01,\n",
      "          2.1642e-01, -2.6634e-01, -8.9576e-01,  1.2211e+00,  1.5316e+00,\n",
      "         -1.0550e-01,  1.4576e+00,  1.4376e+00, -2.3883e-01,  2.4447e-01,\n",
      "         -5.1494e-01, -1.8882e+00,  6.1722e-01, -7.7090e-02,  6.4463e-01,\n",
      "         -6.3426e-01, -2.7871e-01,  8.3683e-01, -1.4347e-01,  5.0025e-01,\n",
      "         -9.2164e-01,  9.4688e-01, -6.7938e-01, -2.0005e-01,  4.9957e-01,\n",
      "         -5.6478e-01, -1.3447e-01,  4.3729e-02,  1.0152e+00,  3.2196e-01,\n",
      "         -1.9983e-01,  2.0087e-01,  1.3855e-01,  8.7111e-03,  2.1781e-01,\n",
      "         -7.3992e-01, -6.3566e-01, -1.6820e+00, -2.7653e+00, -8.6095e-01,\n",
      "         -2.1530e-01, -2.1369e+00, -1.0667e+00, -7.2223e-01, -1.2805e+00,\n",
      "         -3.2830e+00, -2.3454e+00, -3.4593e+00, -3.8096e+00, -2.9950e+00,\n",
      "         -4.1624e+00, -3.0887e+00, -2.2583e+00, -3.3136e+00, -2.4827e+00,\n",
      "         -4.3365e+00, -3.2610e+00, -4.6546e+00, -4.0060e+00, -4.3198e+00,\n",
      "         -3.9669e+00, -3.5899e+00, -2.7144e+00, -3.2283e+00, -3.4824e+00,\n",
      "         -3.2668e+00, -4.5475e+00, -4.3458e+00, -4.4786e+00, -3.1270e+00,\n",
      "         -4.0381e+00, -4.6432e+00, -4.1295e+00, -4.6794e+00, -4.7142e+00,\n",
      "         -4.4376e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "calendar_set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nout_label=[]\\nout_tag=[]\\nfor i in range(intent.shape[0]):\\n    out_label.append(myData.intent_labels[intent[i].argmax()])\\n    out_tag_token = [myData.type_labels[k.argmax()] for k in tag[i]]\\n    print(out_tag_token)\\n    out_tag.append(fill_slot(myData.tokenizer, out_tag_token, test_input[i].tokens[1:]))\\n    \\n    #out_tag.append([myData.type_labels[k.argmax()] for k in tag[i]])\\n    \\nprint(out_label, out_tag)\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModel.eval()\n",
    "myModel.to(device)\n",
    "\n",
    "test_input = \"who are you asdffe\"\n",
    "\n",
    "test_input = myData.tokenizer(test_input, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "\n",
    "intent = myModel(test_input)\n",
    "print(intent)\n",
    "print(myData.intent_labels[intent[i].argmax()])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "out_label=[]\n",
    "out_tag=[]\n",
    "for i in range(intent.shape[0]):\n",
    "    out_label.append(myData.intent_labels[intent[i].argmax()])\n",
    "    out_tag_token = [myData.type_labels[k.argmax()] for k in tag[i]]\n",
    "    print(out_tag_token)\n",
    "    out_tag.append(fill_slot(myData.tokenizer, out_tag_token, test_input[i].tokens[1:]))\n",
    "    \n",
    "    #out_tag.append([myData.type_labels[k.argmax()] for k in tag[i]])\n",
    "    \n",
    "print(out_label, out_tag)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99380a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb774321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e82b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
